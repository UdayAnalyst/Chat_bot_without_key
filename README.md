
# ğŸ§  PDF GPT Chatbot using LangChain + LlamaCpp

This project lets you **ask questions to a PDF document** using local LLMs like **Mistral 7B** and tools like **LangChain**, **LlamaCpp**, and **ChromaDB**.

ğŸš€ Upload any PDF â†’ Chunk it â†’ Embed it â†’ Ask questions â†’ Get smart, contextual answers â€” all offline!

---

## ğŸ”§ Features

- ğŸ“„ Load PDFs using `PyPDFLoader`
- âœ‚ï¸ Split and chunk text with `RecursiveCharacterTextSplitter`
- ğŸ§  Use `sentence-transformers` (MiniLM) for embeddings
- ğŸ—ƒï¸ Store vector embeddings using `ChromaDB`
- ğŸ¤– Answer questions with local model (`Mistral-7B-OpenOrca.Q4_0.gguf`)
- ğŸ§¾ Built with LangChain & LlamaCpp on your local machine

---

## ğŸ“¦ Technologies Used

- `LangChain`
- `LlamaCpp`
- `ChromaDB`
- `sentence-transformers`
- `PyPDFLoader`
- `HuggingFaceEmbeddings`
- `Jupyter Notebook`

---

## ğŸ–¥ï¸ Demo Screenshot

> *(Add a screenshot here showing the notebook answering a question)*

---

## ğŸ§ª How to Run This Project

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/pdf-gpt.git
cd pdf-gpt
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Download Mistral GGUF Model

Place the `mistral-7b-openorca.Q4_0.gguf` file inside the project directory.

You can find models from:  
ğŸ‘‰ https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF

### 4. Run the Notebook

Open Jupyter Notebook:

```bash
jupyter notebook
```

Open `PDF_GPT.ipynb`, update your **PDF file path**, and start asking questions!

---

## ğŸ“ Folder Structure

```bash
ğŸ“ pdf-gpt
 â”£ ğŸ“„ PDF_GPT.ipynb
 â”£ ğŸ“„ requirements.txt
 â”£ ğŸ“„ mistral-7b-openorca.Q4_0.gguf  â† (You must download separately)
 â”— ğŸ“„ README.md
```

---

## ğŸ“Œ To-Do (Optional Enhancements)

- [ ] Add a Streamlit or Gradio UI
- [ ] Add support for multiple PDFs
- [ ] Enable web-based demo

---

## ğŸ“¢ Connect

Made with â¤ï¸ by **Uday Pandey**  
ğŸ“¬ Reach me on [LinkedIn](https://linkedin.com/in/uday-pandey)
